# DeepDream

A containerized approach to making your own deep dream images

## Dependencies

The only dependencies for this project are [singularity](https://sylabs.io/guides/3.6/user-guide/quick_start.html#quick-installation-steps),
Python 3.5 or newer, an nvidia GPU with CUDA support (I'm working on a CPU only version, but it's not done yet), and an internet connection. The actual ML frameworks are
installed and managed by the singularity virtual environment, removing the need to install them on your system.

## Set Up

To download the supported models and build the container with the dependencies, just run `make` and sit back and relax.
The make process is network bottle-necked, so unless you have a multi-gigabit connection, there likely won't be any
benefit of using a `-j` flag.

## Running

To run the model on a set of images, execute `./main.py -s [source image directory] -d [destination image directory]`. The main
program will then automatically bind all the necessary directories to the singularity container and begin execution.

## Arguments accepted

| Flag                 | Short Form | Argument Type | Default               | Use                                                                                                                                                                                                                  |
| -------------------- | ---------- | ------------- | --------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--blend`            | `-b`       | None Taken    | N/A                   | When present, blends in a bit of the previously generated image to the new image. This allows some level of temporal stability when generating deep-dream videos. Assumes that the frames are in ascii-betical order |
| `--destination`      | `-d`       | Directory     | None, must to be set  | Selects the directory that processed images will be written to                                                                                                                                                       |
| `--guide`            | `-g`       | JPEG Image    | None                  | When present, uses the specified image as a stylistic guide. When not present no style guide is used                                                                                                                 |
| `--jitter`           | `-j`       | Integer       | 32                    | Maximum displacement for the random translation applied to the images for each algorithm set                                                                                                                         |
| `--maximize`         | `-m`       | Layer         | `inception_4c/output` | The layer within the neural net to maximize the activation on. Using lower leveled layers will generate more geometric dreams, and higher level layers will generate more recognizable objects                       |
| `--num-iterations`   | `-n`       | Integer       | 1                     | The number of iterations of the algorithm to be ran on each image. The larger this number, the more pronounced the dream like structures will be                                                                     |
| `--octave-count`     |            | Integer       | 4                     | The number of "octaves" to be run on each image. Each octave will detect and bring out patterns at an increasing scale                                                                                               |
| `--octave-scale`     |            | Float         | 1.3                   | The amount to scale the size of features generated by each subsequent octave                                                                                                                                         |
| `--source`           | `-s`       | Directory     | None, must be set     | Selects the directory that raw images will be read from                                                                                                                                                              |
| `--steps-per-octave` |            | Integer       | 10                    | The number of min-max steps to take on each individual octave during image generation. Has a similar effect to `--num-iterations`, but is applied in a different order                                               |

## If you're on Windows or Mac

Uhhhhhhh.... have fun. I make no guarantee that this project will work for you. ¯\\\_(ツ)\_/¯

You can try though! Let me know if you get it working.

## TODO's

- **Multi-GPU**: Add support for multi-GPU. This will be easy when blend is off, but will require some fancy transition-sensitive allocation logic for blended videos 
- **CPU support**: Sure it's slow, but sometimes it's the only option
- **GPU selection**: Select what GPU (or GPUs if I get multi-GPU working) you want the model to use in multi-GPU systems
- **Better blend options**: Right now the blend factor is just randomly selected each frame. It would be nice to include things like constant factors or Gaussian distributions
- **Style transfers**: Spaghetti videos! I need to find the models for this...
- **Windows/Mac support**: This will require some basic restructuring of the `src/dream.py` file to allow it to be run outside of the container, or for windows and mac users to figure out how to run singularity
- **Single Image Support**: For when you only want to process a single image instead of a whole directory
- **Support for other image formats**: Currently the program only supports jpegs... :(
